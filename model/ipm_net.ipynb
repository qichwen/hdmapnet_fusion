{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/qichen/projects/HDMapNet-fusion', '/home/qichen/projects/HDMapNet-fusion/model', '/home/qichen/anaconda3/bin', '/home/qichen/anaconda3/condabin', '/usr/local/sbin', '/usr/local/bin', '/usr/sbin', '/usr/bin', '/sbin', '/bin', '/usr/games', '/usr/local/games', '/snap/bin', '/usr/local/cuda/bin', '/usr/bin/python3', '/home/qichen/projects/MapTR-main', '/home/qichen/anaconda3/envs/hdmapnet_py38/lib/python38.zip', '/home/qichen/anaconda3/envs/hdmapnet_py38/lib/python3.8', '/home/qichen/anaconda3/envs/hdmapnet_py38/lib/python3.8/lib-dynload', '', '/home/qichen/anaconda3/envs/hdmapnet_py38/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "sys.path.insert(0,'/home/qichen/projects/HDMapNet-fusion')\n",
    "print(sys.path)\n",
    "\n",
    "from homography import IPM, bilinear_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plane_grid_2d, get_rot_2d, cam_to_pixel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CamEncode, BevEncode\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .utils import plane_grid_2d, get_rot_2d, cam_to_pixel\n",
    "from .base import CamEncode, BevEncode\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from image_processer import *\n",
    "from data.const import *\n",
    "\n",
    "class IPMNet(nn.Module):\n",
    "    def __init__(self, xbound, ybound, outC, camC=64, instance_seg=True, embedded_dim=16, cam_encoding=True, bev_encoding=True, z_roll_pitch=False):\n",
    "        super(IPMNet, self).__init__()\n",
    "        self.xbound = xbound\n",
    "        self.ybound = ybound\n",
    "        self.camC = camC\n",
    "        self.downsample = 16\n",
    "        if cam_encoding:\n",
    "            self.ipm = IPM(xbound, ybound, N=6, C=camC, z_roll_pitch=z_roll_pitch, extrinsic=False)\n",
    "        else:\n",
    "            self.ipm = IPM(xbound, ybound, N=6, C=camC, visual=True, z_roll_pitch=z_roll_pitch, extrinsic=False)\n",
    "        self.cam_encoding = cam_encoding\n",
    "        if cam_encoding:\n",
    "            self.camencode = CamEncode(camC)\n",
    "        self.bev_encoding = bev_encoding\n",
    "        if bev_encoding:\n",
    "            self.bevencode = BevEncode(inC=camC, outC=outC, instance_seg=instance_seg, embedded_dim=embedded_dim)\n",
    "\n",
    "    def get_cam_feats(self, x):\n",
    "        \"\"\"Return B x N x D x H/downsample x W/downsample x C\n",
    "        \"\"\"\n",
    "        B, N, C, imH, imW = x.shape\n",
    "\n",
    "        x = x.view(B*N, C, imH, imW)\n",
    "        x = self.camencode(x)\n",
    "        x = x.view(B, N, self.camC, imH//self.downsample, imW//self.downsample)\n",
    "        return x\n",
    "\n",
    "    def get_Ks_RTs_and_post_RTs(self, intrins, rots, trans, post_rots, post_trans):\n",
    "        B, N, _, _ = intrins.shape\n",
    "        Ks = torch.eye(4, device=intrins.device).view(1, 1, 4, 4).repeat(B, N, 1, 1)\n",
    "        Ks[:, :, :3, :3] = intrins\n",
    "\n",
    "        Rs = torch.eye(4, device=rots.device).view(1, 1, 4, 4).repeat(B, N, 1, 1)\n",
    "        Rs[:, :, :3, :3] = rots.transpose(-1, -2).contiguous()\n",
    "        Ts = torch.eye(4, device=trans.device).view(1, 1, 4, 4).repeat(B, N, 1, 1)\n",
    "        Ts[:, :, :3, 3] = -trans\n",
    "        RTs = Rs @ Ts\n",
    "\n",
    "        post_RTs = torch.eye(4, device=post_rots.device).view(1, 1, 4, 4).repeat(B, N, 1, 1)\n",
    "        post_RTs[:, :, :3, :3] = post_rots\n",
    "        post_RTs[:, :, :3, 3] = post_trans\n",
    "\n",
    "        if self.cam_encoding:\n",
    "            scale = torch.Tensor([\n",
    "                [1/self.downsample, 0, 0, 0],\n",
    "                [0, 1/self.downsample, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]\n",
    "            ]).cuda()\n",
    "            post_RTs = scale @ post_RTs\n",
    "\n",
    "        return Ks, RTs, post_RTs\n",
    "\n",
    "    def forward(self, points, points_mask, x, rots, trans, intrins, post_rots, post_trans, translation, yaw_pitch_roll):\n",
    "        if self.cam_encoding:\n",
    "            x = self.get_cam_feats(x)\n",
    "\n",
    "        Ks, RTs, post_RTs = self.get_Ks_RTs_and_post_RTs(intrins, rots, trans, post_rots, post_trans)\n",
    "        topdown = self.ipm(x, Ks, RTs, translation, yaw_pitch_roll, post_RTs)\n",
    "\n",
    "        if self.bev_encoding:\n",
    "            return self.bevencode(topdown)\n",
    "        else:\n",
    "            return topdown\n",
    "        \n",
    "class TemporalIPMNet(IPMNet):\n",
    "    def __init__(self, xbound, ybound, outC, camC=64, instance_seg=True, embedded_dim=16):\n",
    "        super(TemporalIPMNet, self).__init__(xbound, ybound, outC, camC, instance_seg, embedded_dim, cam_encoding=True, bev_encoding=True, z_roll_pitch=False)\n",
    "\n",
    "    def get_cam_feats(self, x):\n",
    "        \"\"\"Return B x T x N x H/downsample x W/downsample x C\n",
    "        \"\"\"\n",
    "        B, T, N, C, imH, imW = x.shape\n",
    "        \"\"\"\n",
    "        B: Batch size. This is the number of samples or instances in the batch.\n",
    "        T: Time or sequence length. This dimension often represents a sequence of frames, time steps, or other sequential data.\n",
    "        N: Number of objects, keypoints, or features. This dimension might represent a set of objects, keypoints, or features extracted from an image or video.\n",
    "        C: Number of channels or features. This dimension typically represents the number of color channels (e.g., RGB) or feature channels (e.g., depth, normals) in an image.\n",
    "        imH: Image height. This is the vertical resolution of the image.\n",
    "        imW: Image width. This is the horizontal resolution of the image.\n",
    "        \"\"\"\n",
    "        x = x.view(B*T*N, C, imH, imW)\n",
    "        x = self.camencode(x)\n",
    "        x = x.view(B, T, N, self.camC, imH//self.downsample, imW//self.downsample)\n",
    "        return x\n",
    "\n",
    "    def temporal_fusion(self, topdown, translation, yaw, smp):\n",
    "        #\n",
    "        draw_tensor(smp, topdown)\n",
    "        \n",
    "        B, T, C, H, W = topdown.shape   \n",
    "        \n",
    "        if T == 1:\n",
    "            \n",
    "            return topdown[:, 0]\n",
    "\n",
    "        grid = plane_grid_2d(self.xbound, self.ybound).view(1, 1, 2, H*W).repeat(B, T-1, 1, 1)\n",
    "        rot0 = get_rot_2d(yaw[:, 1:])\n",
    "        trans0 = translation[:, 1:, :2].view(B, T-1, 2, 1)\n",
    "        rot1 = get_rot_2d(yaw[:, 0].view(B, 1).repeat(1, T-1))\n",
    "        trans1 = translation[:, 0, :2].view(B, 1, 2, 1).repeat(1, T-1, 1, 1)\n",
    "        grid = rot1.transpose(2, 3) @ grid\n",
    "        grid = grid + trans1\n",
    "        grid = grid - trans0\n",
    "        grid = rot0 @ grid\n",
    "        grid = grid.view(B*(T-1), 2, H, W).permute(0, 2, 3, 1).contiguous()\n",
    "        grid = cam_to_pixel(grid, self.xbound, self.ybound)\n",
    "        topdown = topdown.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        prev_topdown = topdown[:, 1:]\n",
    "        warped_prev_topdown = bilinear_sampler(prev_topdown.reshape(B*(T-1), H, W, C), grid).view(B, T-1, H, W, C)\n",
    "        topdown = torch.cat([topdown[:, 0].unsqueeze(1), warped_prev_topdown], axis=1)\n",
    "        topdown = topdown.view(B, T, H, W, C)\n",
    "        topdown = topdown.max(1)[0]\n",
    "        topdown = topdown.permute(0, 3, 1, 2).contiguous()\n",
    "        return topdown\n",
    "\n",
    "    # def forward(self, points, points_mask, x, rots, trans, intrins, post_rots, post_trans, translation, yaw_pitch_roll,smp):\n",
    "    def forward(self, x, rots, trans, intrins, post_rots, post_trans, translation, yaw_pitch_roll,smp):\n",
    "        x = self.get_cam_feats(x)\n",
    "        \n",
    "        B, T, N, C, h, w = x.shape\n",
    "        x = x.view(B*T, N, C, h, w)\n",
    "        \n",
    "        # x,counter, sample_token, sample_channel, ts, step\n",
    "        for i in range(x.size(1)):\n",
    "            visualizer_plt(x[0][i].squeeze(0),i, smp[0][0], CAMS[i] , '0', '0', 'plt_images/tempfusion/')\n",
    "        \n",
    "        intrins = intrins.view(B*T, N, 3, 3)\n",
    "        rots = rots.view(B*T, N, 3, 3)\n",
    "        trans = trans.view(B*T, N, 3)\n",
    "        post_rots = post_rots.view(B*T, N, 3, 3)\n",
    "        post_trans = post_trans.view(B*T, N, 3)\n",
    "        \n",
    "        Ks, RTs, post_RTs = self.get_Ks_RTs_and_post_RTs(intrins, rots, trans, post_rots, post_trans)\n",
    "        #TODO: something wrong here in IPM\n",
    "        topdown = self.ipm(x, Ks, RTs, translation, yaw_pitch_roll, post_RTs)\n",
    "        _, C, H, W = topdown.shape\n",
    "        topdown = topdown.view(B, T, C, H, W)\n",
    "        \n",
    "        topdown = self.temporal_fusion(topdown, translation, yaw_pitch_roll[..., 0], smp)\n",
    "        \n",
    "        return self.bevencode(topdown)\n",
    "\n",
    "        # Original hdmapnet \n",
    "        # def forward(self, img, trans, rots, intrins, post_trans, post_rots, lidar_data, lidar_mask, car_trans, yaw_pitch_roll):\n",
    "        # x = self.get_cam_feats(img)\n",
    "        # x = self.view_fusion(x)\n",
    "        # Ks, RTs, post_RTs = self.get_Ks_RTs_and_post_RTs(intrins, rots, trans, post_rots, post_trans)\n",
    "        # topdown = self.ipm(x, Ks, RTs, car_trans, yaw_pitch_roll, post_RTs)\n",
    "        # topdown = self.up_sampler(topdown)\n",
    "        # if self.lidar:\n",
    "        #     lidar_feature = self.pp(lidar_data, lidar_mask)\n",
    "        #     topdown = torch.cat([topdown, lidar_feature], dim=1)\n",
    "        # return self.bevencode(topdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tqdm\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from data.dataset import semantic_dataset\n",
    "from data.const import NUM_CLASSES\n",
    "from model import get_model\n",
    "from postprocess.vectorize import vectorize\n",
    "from data.dataset_TF import *\n",
    "\n",
    "\n",
    "def onehot_encoding(logits, dim=1):\n",
    "    max_idx = torch.argmax(logits, dim, keepdim=True)\n",
    "    one_hot = logits.new_full(logits.shape, 0)\n",
    "    one_hot.scatter_(dim, max_idx, 1)\n",
    "    return one_hot\n",
    "\n",
    "def vis_resutls(args, img_name, batchi, sample_token_str):\n",
    "    batchi = 0\n",
    "    # cluster imge path\n",
    "    img_path = f'plt_images/{sample_token_str}/'\n",
    "    # cluster_path = os.path.join(img_path, 'image_cluster')\n",
    "    if not os.path.exists(img_path):\n",
    "        os.mkdir(img_path)\n",
    "    smaple_token = sample_token_str\n",
    "    # samples = read_json_file(args.image_json)\n",
    "    # for smaple_token in samples:\n",
    "    #TODO: to add try exception logic\n",
    "    BEV_topdown = [f for f in os.listdir(img_path) if (f.startswith('topdown_afterIPM_Usamp_')) and f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))][0]  \n",
    "    BEV_topdown = os.path.join(img_path, BEV_topdown)\n",
    "    \n",
    "    segment_img = [f for f in os.listdir(img_path) if (f.startswith('segmentation_')) and f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))][0]  \n",
    "    segment_img = os.path.join(img_path, segment_img)\n",
    "\n",
    "    vector_img = [f for f in os.listdir(img_path) if (f.startswith('eval_')) and f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))][0]  \n",
    "    vector_img = os.path.join(img_path, vector_img)\n",
    "\n",
    "    overall_img = [f for f in os.listdir(img_path) if (f.startswith('6viewsfused')) and f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))][0]  \n",
    "    overall_img = os.path.join(img_path, overall_img)\n",
    "   \n",
    "    seg_bev_img_ori = [f for f in os.listdir(img_path) if (f.startswith('fm_overall_')) and f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))][0]  \n",
    "    seg_bev_img = os.path.join(img_path, seg_bev_img_ori)\n",
    "    \n",
    "    seg_bev_ori = Image.open(seg_bev_img)\n",
    "    seg_bev = seg_bev_ori.resize((2200, 1200))\n",
    "    \n",
    "    bev_img_cam_ori = Image.open(BEV_topdown)\n",
    "    bev_img_cam = bev_img_cam_ori.resize((2200, 1200))\n",
    "    \n",
    "    # plt.imshow(bev_img_cam)\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(f'BEV_topdown_.jpg')\n",
    "    seg_img_cam_ori = Image.open(segment_img)\n",
    "    seg_img_cam = seg_img_cam_ori.resize((2200, 1200))\n",
    "    # plt.imshow(seg_img_cam)\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(f'seg_.jpg')\n",
    "    vec_img_cam_ori = Image.open(vector_img)\n",
    "    vec_img_cam = vec_img_cam_ori.resize((1600, 1200))\n",
    "    # plt.imshow(vec_img_cam)\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(f'vec_.jpg')\n",
    "    overall_img_ori = Image.open(overall_img)\n",
    "    overall_img_cam = overall_img_ori.resize((6000, 3600))            \n",
    "    # if not os.path.exists(segment_sample_path):\n",
    "    #     os.mkdir(segment_sample_path)\n",
    "    # shutil.copy(segment_img, segment_sample_path)\n",
    "    #\n",
    "    new_cam_image = Image.new('RGB', (10000, 5000))\n",
    "    ###pastes the overall_img image onto the new_cam_image at the specified position (0, 1800). This means that \n",
    "    # the top-left corner of overall_img will be placed at the x-coordinate 0 and the y-coordinate 1800 on the new_cam_image\n",
    "    new_cam_image.paste(overall_img_cam, (0, 1200+1200*2))\n",
    "    new_cam_image.paste(bev_img_cam, (0, 0))\n",
    "    new_cam_image.paste(seg_bev, (2200, 0))\n",
    "    new_cam_image.paste(seg_img_cam, (4400, 0))\n",
    "    new_cam_image.paste(vec_img_cam, (6600, 0))\n",
    "    \n",
    "    raw_imgs = sorted([f for f in os.listdir(img_path) if (((('fm_imgafter_norm') in f) or ('pvimg_before_norm' in f)) and f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')))])  \n",
    "    for imgn, image_file in enumerate(raw_imgs):\n",
    "        rows = imgn//6\n",
    "        rols = imgn%6  \n",
    "        image_path = os.path.join(img_path, image_file)                  \n",
    "        pv = Image.open(image_path).resize((1600,1200)) \n",
    "        new_cam_image.paste(pv, (rols * 1600, 1200+1200*rows))      \n",
    "\n",
    "    img_dir = f'cluster_result'\n",
    "    if not os.path.exists(img_dir):\n",
    "        os.mkdir(img_dir)\n",
    "    \n",
    "    img_name = f'cluster_{batchi:06}_{smaple_token}.jpg'    \n",
    "    new_image_path = os.path.join(img_dir, img_name)   \n",
    "    new_cam_image.save(new_image_path)\n",
    "    shutil.copy(new_image_path, img_path)\n",
    "    plt.close()\n",
    "    # batchi = batchi + 1    \n",
    "    print(f\"exported to {new_image_path}\")\n",
    "    \n",
    "def vis_segmentation(model, val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        segment_path = os.path.join(os.getcwd(), 'segment_result')\n",
    "        \n",
    "\n",
    "        if not os.path.exists(segment_path):\n",
    "            os.mkdir(segment_path)\n",
    "        for batchi, (imgs, rots, trans, intrins, post_rots, post_trans, car_trans, yaw_pitch_roll, sample_token) in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                semantic, embedding, direction = model(imgs.cuda(), rots.cuda(),trans.cuda(), intrins.cuda(),\n",
    "                                                    post_rots.cuda(), post_trans.cuda(), car_trans.cuda(), yaw_pitch_roll.cuda(), sample_token)\n",
    "                semantic = semantic.softmax(1).cpu().numpy()\n",
    "            else:\n",
    "                semantic, embedding, direction = model(imgs, trans, rots, intrins,\n",
    "                                                    post_trans, post_rots, lidar_data,\n",
    "                                                    lidar_mask, car_trans, yaw_pitch_roll,sample_token[0])\n",
    "                semantic = semantic.softmax(1).numpy()\n",
    "            # compare with gt, set NaN to non-valid value? #TODO\n",
    "            # print(semantic_gt.shape)\n",
    "            # print(semantic.shape)\n",
    "            # print(\"======shape of gt, then semantic as above======\")\n",
    "\n",
    "            # print(semantic_gt)\n",
    "            # print(\"======semantic_gt above======\")\n",
    "            # print(semantic_gt < 0.1)\n",
    "            # print(\"======semantic_gt < 0.1 above======\")\n",
    "            # print(semantic[semantic_gt < 0.1])\n",
    "            # print(\"======output semantic_gt < 0.1 above======\")\n",
    "            # print(semantic_gt[0][0][199])\n",
    "            #semantic[semantic_gt < 0.1] = np.nan\n",
    "            sample_token_str = sample_token[0][0]\n",
    "            pltimage_dir = os.path.join('plt_images', sample_token_str)\n",
    "            if not os.path.exists(pltimage_dir):\n",
    "                os.mkdir(pltimage_dir)\n",
    "            for si in range(semantic.shape[0]):\n",
    "                #print(len(semantic[0][1]))\n",
    "                #print(semantic[0][1])\n",
    "                plt.figure(figsize=(4, 2), dpi=400)\n",
    "                \n",
    "                plt.imshow(semantic[si][0], alpha=1)\n",
    "                plt.xlim(0, 400)\n",
    "                plt.ylim(0, 200)\n",
    "                #plt.axis('off')\n",
    "                imname = f'segment_[0][0]_other_{sample_token_str}.jpg'\n",
    "                image_path = os.path.join(pltimage_dir, imname)                \n",
    "                print('saving', image_path)                \n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "                \n",
    "                plt.imshow(semantic[si][1], vmin=0, cmap='Blues', vmax=1, alpha=1)\n",
    "                plt.xlim(0, 400)\n",
    "                plt.ylim(0, 200)\n",
    "                #plt.axis('off')\n",
    "                imname = f'segment_[0][1]_blues_{sample_token_str}.jpg'\n",
    "                image_path = os.path.join(pltimage_dir, imname)                \n",
    "                print('saving', image_path)\n",
    "                \n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "                \n",
    "                plt.imshow(semantic[si][2], vmin=0, cmap='Reds', vmax=1, alpha=1)\n",
    "                plt.xlim(0, 400)\n",
    "                plt.ylim(0, 200)\n",
    "                #plt.axis('off')\n",
    "                imname = f'segment_[0][2]_reds_{sample_token_str}.jpg'\n",
    "                image_path = os.path.join(pltimage_dir, imname)                \n",
    "                print('saving', image_path)\n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "                \n",
    "                plt.imshow(semantic[si][3], vmin=0, cmap='Greens', vmax=1, alpha=1)\n",
    "                plt.xlim(0, 400)\n",
    "                plt.ylim(0, 200)\n",
    "                plt.axis('off')\n",
    "                imname = f'segment_[0][3]_greens_{sample_token_str}.jpg'\n",
    "                image_path = os.path.join(pltimage_dir, imname)                \n",
    "                print('saving', image_path)\n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "\n",
    "                # #seg gt overall geometry\n",
    "                # plt.imshow(semantic_gt[si][0],alpha=1)\n",
    "                # plt.xlim(0, 400)\n",
    "                # plt.ylim(0, 200)\n",
    "                # plt.axis('off')\n",
    "                # imname = f'segmentGT_[0][0]_{sample_token_str}.jpg'\n",
    "                # image_path = os.path.join(pltimage_dir, imname)                \n",
    "                # print('saving', image_path)\n",
    "                # plt.savefig(image_path)\n",
    "                # plt.close()\n",
    "                \n",
    "                # # road / lane divider\n",
    "                # plt.imshow(semantic_gt[si][1], alpha=1)\n",
    "                # plt.xlim(0, 400)\n",
    "                # plt.ylim(0, 200)\n",
    "                # plt.axis('off')\n",
    "                # imname = f'segmentGT_[0][1]_{sample_token_str}.jpg'\n",
    "                # image_path = os.path.join(pltimage_dir, imname)                \n",
    "                # print('saving', image_path)\n",
    "                # plt.savefig(image_path)\n",
    "                # plt.close()\n",
    "                \n",
    "                # # pedestrain crossing\n",
    "                # plt.imshow(semantic_gt[si][2], alpha=1)\n",
    "                # plt.xlim(0, 400)\n",
    "                # plt.ylim(0, 200)\n",
    "                # plt.axis('off')\n",
    "                # imname = f'segmentGT_[0][2]_{sample_token_str}.jpg'\n",
    "                # image_path = os.path.join(pltimage_dir, imname)                \n",
    "                # print('saving', image_path)\n",
    "                # plt.savefig(image_path)\n",
    "                # plt.close()\n",
    "                \n",
    "                # # contour\n",
    "                # plt.imshow(semantic_gt[si][3], alpha=1)\n",
    "                # plt.xlim(0, 400)\n",
    "                # plt.ylim(0, 200)\n",
    "                # plt.axis('off')\n",
    "                # imname = f'segmentGT_[0][3]_{sample_token_str}.jpg'\n",
    "                # image_path = os.path.join(pltimage_dir, imname)                \n",
    "                # print('saving', image_path)\n",
    "                # plt.savefig(image_path)\n",
    "                # plt.close()\n",
    "                \n",
    "                # fig.axes.get_xaxis().set_visible(False)\n",
    "                # fig.axes.get_yaxis().set_visible(False)\n",
    "                plt.xlim(0, 400)\n",
    "                plt.ylim(0, 200)\n",
    "                #plt.axis('off')\n",
    "                #TODO: print seg gt\n",
    "                imname = f'eval_segment_{batchi:06}_{sample_token_str}_{si:03}.jpg'\n",
    "                image_path = os.path.join(segment_path, imname)\n",
    "                print('saving', image_path)\n",
    "                plt.savefig(image_path)\n",
    "                \n",
    "                #print(os.path.exists(pltimage_dir))\n",
    "                \n",
    "                pltimage_path = f'{pltimage_dir}/segmentation_{batchi:06}_{sample_token_str}_{si:03}.jpg'\n",
    "                plt.savefig(pltimage_path)\n",
    "                \n",
    "                # fig.clf()\n",
    "\n",
    "                plt.close()                \n",
    "                gc.collect()\n",
    "            # del semantic\n",
    "            gc.collect()\n",
    "    \n",
    "\n",
    "\n",
    "def vis_vector(model, val_loader, angle_class):\n",
    "    model.eval()\n",
    "    car_img_path = os.path.join(os.getcwd(), 'icon', 'car.png')\n",
    "    car_img = Image.open(car_img_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vector_path = os.path.join(os.getcwd(), 'vector_result')\n",
    "        if not os.path.exists(vector_path):\n",
    "            os.mkdir(vector_path)\n",
    "        for batchi, (imgs, trans, rots, intrins, post_trans, post_rots, lidar_data, lidar_mask, car_trans, yaw_pitch_roll, segmentation_gt, instance_gt, direction_gt, sample_token) in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                segmentation, embedding, direction = model(imgs.cuda(), trans.cuda(), rots.cuda(), intrins.cuda(),\n",
    "                                                       post_trans.cuda(), post_rots.cuda(), lidar_data.cuda(),\n",
    "                                                       lidar_mask.cuda(), car_trans.cuda(), yaw_pitch_roll.cuda(), sample_token[0] )\n",
    "            else:\n",
    "                segmentation, embedding, direction = model(imgs, trans, rots, intrins,\n",
    "                                                       post_trans, post_rots, lidar_data,\n",
    "                                                       lidar_mask, car_trans, yaw_pitch_roll, sample_token[0])\n",
    "            sample_token_str = sample_token[0]\n",
    "            print(segmentation.shape)\n",
    "            \n",
    "            # si is the batch size, == 0 in debug.\n",
    "            for si in range(segmentation.shape[0]):\n",
    "                coords, _, _ = vectorize(segmentation[si], embedding[si], direction[si], angle_class)\n",
    "\n",
    "                for coord in coords:\n",
    "                    #draw lines based on its x , y and lane width\n",
    "                    plt.plot(coord[:, 0], coord[:, 1], linewidth=5)\n",
    "\n",
    "                plt.xlim((0, segmentation.shape[3]))\n",
    "                plt.ylim((0, segmentation.shape[2]))\n",
    "                plt.imshow(car_img, extent=[segmentation.shape[3]//2-15, segmentation.shape[3]//2+15, segmentation.shape[2]//2-12, segmentation.shape[2]//2+12])\n",
    "\n",
    "                # img_name = f'eval_vector_{batchi:06}_{sample_token_str}_{si:03}.jpg'\n",
    "                # image_path = os.path.join(vector_path, img_name)\n",
    "                # print('saving', image_path)\n",
    "                # plt.savefig(image_path)\n",
    "                # plt.close()\n",
    "                img_name = f'plt_images/{sample_token_str}/eval_vector{batchi:06}_{sample_token_str}_{si:03}.jpg'                \n",
    "                print('saving', img_name)\n",
    "                plt.savefig(img_name)\n",
    "                plt.close()\n",
    "                \n",
    "                # vis_resutls(args, img_name, batchi, sample_token_str)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    data_conf = {\n",
    "        'num_channels': NUM_CLASSES + 1,\n",
    "        'image_size': args.image_size,\n",
    "        'xbound': args.xbound,\n",
    "        'ybound': args.ybound,\n",
    "        'zbound': args.zbound,\n",
    "        'dbound': args.dbound,\n",
    "        'thickness': args.thickness,\n",
    "        'angle_class': args.angle_class,\n",
    "    }\n",
    "\n",
    "    val_loader_tf = dataset_tf(args.version, args.dataroot, data_conf, args.bsz, args.nworkers, args.tf_size)\n",
    "    model = get_model(args.model, data_conf, args.instance_seg, args.embedding_dim, args.direction_pred, args.angle_class)\n",
    "    model.load_state_dict(torch.load(args.modelf), strict=False)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        \n",
    "    vis_segmentation(model, val_loader_tf)\n",
    "    # vis_vector(model, val_loader, args.angle_class)\n",
    "    # vis_segmentation(model, val_loader)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # logging config\n",
    "    parser.add_argument(\"--logdir\", type=str, default='./runs')\n",
    "\n",
    "    # nuScenes config\n",
    "    parser.add_argument('--dataroot', type=str, default='dataset/nuScenes/')\n",
    "    parser.add_argument('--version', type=str, default='v1.0-mini', choices=['v1.0-trainval', 'v1.0-mini', 'mb_test'])\n",
    "\n",
    "    # model config\n",
    "    parser.add_argument(\"--model\", type=str, default='HDMapNet_cam')\n",
    "\n",
    "    # training config\n",
    "    parser.add_argument(\"--nepochs\", type=int, default=30)\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=5.0)\n",
    "    parser.add_argument(\"--pos_weight\", type=float, default=2.13)\n",
    "    parser.add_argument(\"--bsz\", type=int, default=1)\n",
    "    parser.add_argument(\"--nworkers\", type=int, default=0)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-7)\n",
    "\n",
    "    # finetune config\n",
    "    parser.add_argument('--finetune', action='store_true')\n",
    "    parser.add_argument('--modelf', type=str, default=None)\n",
    "\n",
    "    # data config\n",
    "    parser.add_argument(\"--thickness\", type=int, default=5)\n",
    "    parser.add_argument(\"--image_size\", nargs=2, type=int, default=[128, 352])\n",
    "    parser.add_argument(\"--xbound\", nargs=3, type=float, default=[-30.0, 30.0, 0.15])\n",
    "    parser.add_argument(\"--ybound\", nargs=3, type=float, default=[-15.0, 15.0, 0.15])\n",
    "    parser.add_argument(\"--zbound\", nargs=3, type=float, default=[-10.0, 10.0, 20.0])\n",
    "    parser.add_argument(\"--dbound\", nargs=3, type=float, default=[4.0, 45.0, 1.0])\n",
    "    parser.add_argument(\"--tf_size\", type=int, default=4)\n",
    "\n",
    "\n",
    "    # embedding config\n",
    "    parser.add_argument('--instance_seg', action='store_true')\n",
    "    parser.add_argument(\"--embedding_dim\", type=int, default=16)\n",
    "    parser.add_argument(\"--delta_v\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--delta_d\", type=float, default=3.0)\n",
    "\n",
    "    # direction config\n",
    "    parser.add_argument('--direction_pred', action='store_true')\n",
    "    parser.add_argument('--angle_class', type=int, default=36)\n",
    "\n",
    "    # loss config\n",
    "    parser.add_argument(\"--scale_seg\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--scale_var\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--scale_dist\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--scale_direction\", type=float, default=0.2)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdmapnet_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
